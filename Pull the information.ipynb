{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b25269-9736-4ab8-9b0b-5586f6a0542d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n|              Region|Average Temperature|\n+--------------------+-------------------+\n|              Africa|  74.40260231125495|\n|                Asia|   68.1097225987458|\n|              Europe|  51.94717142841552|\n|Australia/South P...|  62.30369323842191|\n|       North America|  56.15019771858279|\n|         Middle East|  73.84068255374054|\n|South/Central Ame...|  72.20202379397276|\n+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Question1 A\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"AvgTempByRegionDataFrame\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "hw3_df = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/city_temperature.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter out invalid temperature entries\n",
    "filtered_df = hw3_df.filter(col(\"AvgTemperature\") != -99)\n",
    "\n",
    "# Compute the average temperature for each region\n",
    "average_by_region_df = filtered_df.groupBy(\"Region\").agg(avg(\"AvgTemperature\").alias(\"Average Temperature\"))\n",
    "\n",
    "# Display the results\n",
    "average_by_region_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b9da20-eb74-4f9b-b9b2-f4b59e3411ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n|Year|Average Temperature|\n+----+-------------------+\n|1995|   67.2629851012579|\n|1996|  67.86967758228818|\n|1997|  68.34836746936332|\n|1998|  69.06291989664082|\n|1999|  68.02917794316642|\n|2000|  67.71886102847435|\n|2001|  67.73726484541146|\n|2002|  68.05932217366207|\n|2003|   67.9122232063773|\n|2004|  68.31447879572664|\n|2005|  67.78168187744457|\n|2006|  68.70920807327536|\n|2007|  68.87105562784646|\n|2008|   68.0895085896924|\n|2009|  68.72838223632029|\n|2010|  68.82422924901184|\n|2011|  67.65085130533478|\n|2012|  67.23867893253757|\n|2013|  67.83878858474075|\n|2014|   67.8062718640678|\n+----+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Question1 B\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"AvgTempByYearInAsiaDataFrame\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "hw3_df = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/city_temperature.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter rows based on valid AvgTemperature and \"Asia\" region\n",
    "valid_temps_in_asia_df = hw3_df.filter((col(\"AvgTemperature\") != -99.0) & (col(\"Region\") == \"Asia\"))\n",
    "\n",
    "# Compute average temperature for each year\n",
    "average_by_year_df = valid_temps_in_asia_df.groupBy(\"Year\").agg(avg(\"AvgTemperature\").alias(\"Average Temperature\"))\n",
    "\n",
    "# Sort the results by year\n",
    "sorted_results_df = average_by_year_df.orderBy(\"Year\")\n",
    "\n",
    "# Display the results\n",
    "sorted_results_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d2a72d-caa8-4abd-838e-d122766f5e56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n|     City|Average Temperature|\n+---------+-------------------+\n|   Madrid|  58.91773111062988|\n|   Bilbao| 59.173121887854684|\n|Barcelona|  61.78984408835005|\n+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Question1 C\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"AvgTempByCityInSpainDataFrame\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "hw3_df = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/city_temperature.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter rows based on the \"Spain\" country and valid AvgTemperature entries\n",
    "valid_temps_in_spain_df = hw3_df.filter((col(\"Country\") == \"Spain\") & (col(\"AvgTemperature\") != -99.0))\n",
    "\n",
    "# Compute average temperature for each city in Spain\n",
    "average_by_city_df = valid_temps_in_spain_df.groupBy(\"City\").agg(avg(\"AvgTemperature\").alias(\"Average Temperature\"))\n",
    "\n",
    "# Display the results\n",
    "average_by_city_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5145a1e-3373-40d5-a2db-ae9033c7c955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+\n|             Country|Capital City|Average_Temperature|\n+--------------------+------------+-------------------+\n|             Albania|      Tirana| 61.135237970711394|\n|             Algeria|     Algiers|  64.37253818654507|\n|           Argentina|Buenos Aires|  62.91649875419788|\n|           Australia|    Canberra|   56.4014755343388|\n|             Austria|      Vienna|  51.46943722943731|\n|             Bahamas|      Nassau|  78.56622639449847|\n|             Bahrain|      Manama|  81.21907752273744|\n|          Bangladesh|       Dhaka|  78.72032520325234|\n|            Barbados|  Bridgetown|  81.07108635764779|\n|             Belarus|       Minsk|  45.39047039291643|\n|             Belgium|    Brussels|   51.5118640398354|\n|             Bermuda|    Hamilton|  71.59983807124873|\n|             Bolivia|      La Paz|  45.50655826558265|\n|            Bulgaria|       Sofia|   51.6243039116221|\n|             Burundi|   Bujumbura|  73.68981900452471|\n|              Canada|      Ottawa|  44.22525751375754|\n|Central African R...|      Bangui|  78.82184718529616|\n|               China|     Beijing|    55.202587420158|\n|             Croatia|      Zagreb|  54.41298082378286|\n|                Cuba|      Havana|  75.84939520562996|\n+--------------------+------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Question1 D\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "#Load the CSV files into DataFrames\n",
    "df_temp = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/city_temperature.csv\", header=True, inferSchema=True) # Change path as per your file location\n",
    "df_countries = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/country_list.csv\", header=True, inferSchema=True) \n",
    "\n",
    "#Preprocessing on temperature data:\n",
    "#1. Remove rows where AvgTemperature is None or a non-numeric value\n",
    "#2. Handle specific values that are known to be erroneous for temperature (e.g., -99)\n",
    "df_temp_cleaned = df_temp.filter(\n",
    "    col(\"AvgTemperature\").isNotNull() & \n",
    "    (col(\"AvgTemperature\") != -99)\n",
    ")\n",
    "\n",
    "#Join the two DataFrames to get only the rows of capital cities\n",
    "joined_df = df_temp_cleaned.join(df_countries, (df_temp_cleaned[\"Country\"] == df_countries[\"Country\"]) & (df_temp_cleaned[\"City\"] == df_countries[\"Capital\"]))\n",
    "\n",
    "#Group by Country and Capital City to compute average for AvgTemperature\n",
    "avg_temp_by_capital = joined_df.groupBy(df_countries[\"Country\"], df_countries[\"Capital\"].alias(\"Capital City\")).agg(avg(\"AvgTemperature\").alias(\"Average_Temperature\"))\n",
    "\n",
    "#Display the results\n",
    "avg_temp_by_capital.orderBy(\"Country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ad385e-b453-44cc-a861-ae9f81075095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+\n|             Country|Capital City|Average_Temperature|\n+--------------------+------------+-------------------+\n|             Albania|      Tirana| 61.135237970711394|\n|             Algeria|     Algiers|  64.37253818654507|\n|           Argentina|Buenos Aires|  62.91649875419788|\n|           Australia|    Canberra|   56.4014755343388|\n|             Austria|      Vienna|  51.46943722943731|\n|             Bahamas|      Nassau|  78.56622639449847|\n|             Bahrain|      Manama|  81.21907752273744|\n|          Bangladesh|       Dhaka|  78.72032520325234|\n|            Barbados|  Bridgetown|  81.07108635764779|\n|             Belarus|       Minsk|  45.39047039291643|\n|             Belgium|    Brussels|   51.5118640398354|\n|             Bermuda|    Hamilton|  71.59983807124873|\n|             Bolivia|      La Paz|  45.50655826558265|\n|            Bulgaria|       Sofia|   51.6243039116221|\n|             Burundi|   Bujumbura|  73.68981900452471|\n|              Canada|      Ottawa|  44.22525751375754|\n|Central African R...|      Bangui|  78.82184718529616|\n|               China|     Beijing|    55.202587420158|\n|             Croatia|      Zagreb|  54.41298082378286|\n|                Cuba|      Havana|  75.84939520562996|\n+--------------------+------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Question1 E\n",
    "from pyspark.sql.functions import col, avg, broadcast\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_temp = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/city_temperature.csv\", header=True, inferSchema=True)  # Change path as per your file location\n",
    "df_countries = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/country_list.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Preprocessing on temperature data:\n",
    "# 1. Remove rows where AvgTemperature is None or a non-numeric value\n",
    "# 2. Handle specific values that are known to be erroneous for temperature (e.g., -99)\n",
    "df_temp_cleaned = df_temp.filter(\n",
    "    col(\"AvgTemperature\").isNotNull() &\n",
    "    (col(\"AvgTemperature\") != -99)\n",
    ")\n",
    "\n",
    "# Join the two DataFrames to get only the rows of capital cities\n",
    "# Use broadcast join for optimization\n",
    "joined_df = df_temp_cleaned.join(broadcast(df_countries), (df_temp_cleaned[\"Country\"] == df_countries[\"Country\"]) & (df_temp_cleaned[\"City\"] == df_countries[\"Capital\"]))\n",
    "\n",
    "# Group by Country and Capital City to compute average for AvgTemperature\n",
    "avg_temp_by_capital = joined_df.groupBy(df_countries[\"Country\"], df_countries[\"Capital\"].alias(\"Capital City\")).agg(avg(\"AvgTemperature\").alias(\"Average_Temperature\"))\n",
    "\n",
    "# Display the results\n",
    "avg_temp_by_capital.orderBy(\"Country\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40dcd31-ed85-4661-8d52-77f3db9912e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n|Formatted_Output                                                                                  |\n+--------------------------------------------------------------------------------------------------+\n|Tirana is the capital of Albania and its average temperature is 61.01525893104418                 |\n|Algiers is the capital of Algeria and its average temperature is 64.39437550579977                |\n|Buenos Aires is the capital of Argentina and its average temperature is 63.08454999325328         |\n|Canberra is the capital of Australia and its average temperature is 56.64297665632144             |\n|Vienna is the capital of Austria and its average temperature is 51.83600215662492                 |\n|Nassau is the capital of Bahamas and its average temperature is 78.89768864717884                 |\n|Manama is the capital of Bahrain and its average temperature is 81.3842346594743                  |\n|Dhaka is the capital of Bangladesh and its average temperature is 78.79319081551886               |\n|Bridgetown is the capital of Barbados and its average temperature is 81.08751338944161            |\n|Minsk is the capital of Belarus and its average temperature is 45.726364643945814                 |\n|Brussels is the capital of Belgium and its average temperature is 51.66530942429563               |\n|Hamilton is the capital of Bermuda and its average temperature is 71.48836608066183               |\n|La Paz is the capital of Bolivia and its average temperature is 45.576472177201595                |\n|Sofia is the capital of Bulgaria and its average temperature is 51.945604099244804                |\n|Bujumbura is the capital of Burundi and its average temperature is 63.21291390728487              |\n|Ottawa is the capital of Canada and its average temperature is 44.121952145841156                 |\n|Bangui is the capital of Central African Republic and its average temperature is 78.87307532826536|\n|Beijing is the capital of China and its average temperature is 55.138848435814346                 |\n|Zagreb is the capital of Croatia and its average temperature is 54.77584110255372                 |\n|Havana is the capital of Cuba and its average temperature is 75.78236970684006                    |\n+--------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Question1 F\n",
    "from pyspark.sql.functions import col, avg, udf\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "\n",
    "# Define the UDF for year filtering\n",
    "@udf(BooleanType())\n",
    "def filter_year(year):\n",
    "    return year >= 2000\n",
    "\n",
    "# Define the UDF for output formatting\n",
    "@udf(StringType())\n",
    "def format_output(capital, country, average_temperature):\n",
    "    return f\"{capital} is the capital of {country} and its average temperature is {average_temperature}\"\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_temp = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/city_temperature.csv\", header=True, inferSchema=True)\n",
    "df_countries = spark.read.csv(\"/FileStore/shared_uploads/hyeongjinjoo82@gmail.com/country_list.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess the temperature data\n",
    "df_temp_cleaned = df_temp.filter(\n",
    "    col(\"AvgTemperature\").isNotNull() & \n",
    "    (col(\"AvgTemperature\") != -99)\n",
    ").filter(filter_year(col(\"Year\")))\n",
    "\n",
    "# Join the two DataFrames to get only the rows of capital cities, aliasing the columns from df_countries\n",
    "df_countries_aliased = df_countries.withColumnRenamed(\"Country\", \"CountryName\").withColumnRenamed(\"Capital\", \"CapitalName\")\n",
    "joined_df = df_temp_cleaned.join(broadcast(df_countries_aliased), (df_temp_cleaned[\"Country\"] == df_countries_aliased[\"CountryName\"]) & (df_temp_cleaned[\"City\"] == df_countries_aliased[\"CapitalName\"]))\n",
    "\n",
    "# Group by Country and Capital City to compute the average temperature, using the aliased names to avoid ambiguity\n",
    "avg_temp_by_capital = joined_df.groupBy(\"CountryName\", \"CapitalName\").agg(avg(\"AvgTemperature\").alias(\"Average_Temperature\"))\n",
    "\n",
    "# Format the final output using the UDF\n",
    "final_output = avg_temp_by_capital.withColumn(\"Formatted_Output\", format_output(col(\"CapitalName\"), col(\"CountryName\"), col(\"Average_Temperature\")))\n",
    "\n",
    "# Select only the formatted output and order by CountryName\n",
    "final_output.select(\"Formatted_Output\").orderBy(\"CountryName\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-10-30 00:22:47",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
